## S3 ##
- S3 is a fully managed object storage based system: `Highly Available`, `Highly durable`, `Highly scalable` & `cost effective`
- file size supported: 0 bytes - 5 terabytes
- objects stored in S3 have a `durability` of 11 9's as data is replicated across multiple AZs

> Durability: the probability of maintaining your data without it being lost, corruption, degradation of data

- the `availability` of S3 data objects is 4 9's

> Availability: Up time of the service maintaining your data

`Object metadata`: The metadata is similar to a Key (name), Content Type, Storage Type, Date Modified, Version ID, etc

Buckets:
- data is stored in S3 buckets as objects accessible via HTTP/HTTS
- bucket names must be globally unique, buckets are regional based
- default: <100 buckets per AWS account
- names up to 63 characters, only lower case, no period character (bucket name converts to a subdomain when using "." that can't be used with SSL) - bucket name gets converted into DNS
- can be versioned enabled
- allows cross regional replication: CRR to a different region

> S3 automatically partitions buckets according to the prefix of the files -> adding a random value to the key can improve performance

> add a hex hash to the prefix to reduce the chance a request reads from the same partition multiple times

Storage Classes:
- Standard: 11 9's Durability, 4 9's Availability
- Standard-Infrequent Access: 11 9's Durability, 3 9's Availability
- Reduced Reduced Redundancy: 4 9's Durability, 4 9's Availability

Security:
- Bucket policies: set access controls for accesing data in the bucket associated with - permissions for AWS users
- Access Control Lists: only control access from users outside the AWS account - public access
- Data Encryption: server side encryption/ client side encryption

Data management:
- Versioning: 
  * allows multiple versions of the same object to exist
  * recover from accidental deletion
  * versioning is not enabled by default
  * once enabled, versioning can't be disabled - only suspended
  * adds a cost for storing multiple versions of the same object
- Lifesycle rules:
  * provides an automatic method of managing the lifecycle of your data stored - move data between storage classes

Use cases:
- data backup - for existing AWS services/ on-premises data
- static content & websites
- large data sets (computational, mathematical & scientific data)
- integration with EBS (store snapshots), CloudTrail (log files are stored in S3), CloudFront (S3 bucket can be used as a CloudFront origin within a distribution)

Pricing:
- the more storage used, the cost of eeach Gb reduces when a certain treshold is reached
- others costs: requests (based on actions: PUT, GET, COPY, POST), data transfers

Anti-patterns:
- data archiving for long term use
- dynamic & fast changing data
- file system requirements
- structured data with queries

- key in S3 = path 
- version enabled files needs to be all individually deleted (including versions)
- encryption in transit HTTPS
- encryption at rest
- CORS needs to be configured to a bucket

Operations on Objects: PUT, GET, LIST keys, DELETE, RESTORE (restore an object previosuly archived on Glacier)

Performance (https://minops.com/blog/2015/01/aws-s3-performance-tuning/)
- Throughput
  * S3 takes the first few characters of the key and decides which cluster partition to put them in
  * > 3k transactions per second generate a hash prefix for the key

- Requests
  * avoid unnecessary requests: do not check for the existance of a bucket

- Network Latency
  * CDN in front of S3 to distribute content
  * choose the region closest to low latency client
  
  Best pracitices:
  - separate input/output buckets

> When uploading objects using the REST API, the optional user-defined metadata names must begin with "x-amz-meta-" to distinguish them from other HTTP headers

## Static Website ##
- configure an Amazon S3 bucket for website hosting and upload the website content to the bucket
- The website is then available at: `<bucket-name>.s3-website-<AWS-region>.amazonaws.com`
